# Netflix Subscriber Churn

This project applies a series of machine learning models to predict subscriber churn using a dataset modeled after Netflix's user base. In addition to model development, I perform feature engineering to identify the key factors that influence whether a subscriber is likely to churn. 

My interest in this topic stems from both personal experience, like many others, my Netflix usage surged during the COVID-19 pandemic, and the practical relevance of churn prediction in real-world subscription-model businesses. Companies such as Netflix likely rely on data-driven retention models, and this project represents a scaled-down version of how such analytics work in practice. 

This project focuses on the end-to-end machine learning workflow, including data preparation, cleaning and transformation, feature engineering, and the development, evaluation, and interpretation of classification models, and visualization of predicted churn against actual churn using test data (~20% of rows).

Key objectives of this project include:   
  - Determining which machine learning model is most appropriate for the churn-prediction task, balancing accuracy with model complexity and interpretability.  
  - Identifying the key factors that influence wheter a subscriber is likely to churn.  
  
This dataset is a publicly available on [Kaggle](https://www.kaggle.com/datasets/abdulwadood11220/netflix-customer-churn-dataset?resource=download&select=netflix_customer_churn.csv).  

## Dataset Description
The dataset contains over 5,000 synthetic customer records, representing subscriber behaviour for a video-streaming service modeled after Netflix. 

Key columns include: 
  - ``customer_id``
  - ``Churned``

*Additional features will be detailed in later sections.*

## Objective 
The main objective of this project is to develop a machine learning based on historical churn information that is useful for decision making. For the model to be "useful", an emphasis will be placed on interpretability and accurate, as defined by model evaluation measures. 

## Approach
1. Data Cleaning & Preprocessing: loading, cleaning, and preparing the data by addressing outliers, filling in missing values, removing duplicates.
2. Data Visualization: analyze variables, examine potential relationships and correlations between variables through visualizations  
3. Feature Engineering: creating additional features from existing features
4. Model Development: create, train, test, select, and refine machine learning model
5. Model Evaluation & Interpretation: data & model evaluation, model benchmarking 

## Key Insights
*This section is currently in progress and will be updated shortly.*

<!---- Total sales volume and average home prices more than doubled over the 20-year period.-->
<!---- Real estate activity peaked during 2020-2022, a period largely influenced by the COVID-19 pandemic and low interest rates.-->  
<!---- Vancouver was the most popular city for real estate, with Downtown Vancouver & Downtown New Westminster ranking as the top neighbourhoods.-->     
<!---- Spring & Summer consisntely showed the highest sales activity within the year with clear seasonal peaks observed across multiple years-->  
<!--- Properties built after 2010 commanded a higher premium than older homes-->
<!--Key findings and visualizations will be summarized here once analysis is complete.-->

## Next Steps
*This section is currently in progress and will be updated shortly.*
